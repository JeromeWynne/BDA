---
title: "BDA 15: Multiparameter Models"
output: pdf_document
---

Often we'll have to deal with models containing multiple unknowns. It's usually the case that within these models we're only interested in the posterior distribution of one parameter, which we call the marginal posterior distribution of that parameter.
\par
To obtain the marginal posterior it's first necessary to model the joint posterior distribution over all unknowns. We can then integrate over the other unknowns to get the desired marginal posterior.
\par
The computational equivalent of this procedure is to draw many samples from the joint posterior, and record the parameter of interest each time. In doing this, it's possible to build up a frequency distribution that approximates the marginal posterior of the parameter.
\par
Consider the case where we seek to model the joint distribution of two parameters. We can get hold of the unnormalized posterior by taking the product of the joint prior and the sampling distribution:
\begin{equation}
  p(\theta_1, \theta_2 | y) \ \propto \ p(y|\theta_1, \theta_2)\cdot p(\theta_1, \theta_2)
\end{equation}
Let's say we want the marginal posterior of $\theta_1$, $p(\theta_1|y)$. We can get this by integrating the joint posterior over $\theta_2$
\begin{equation}
  p(\theta_1|y) \ = \ \int p(\theta_1, \theta_2 | y)\cdot d\theta_2
\end{equation}
We can re-express the right hand side in terms of a conditional posterior and another marginal posterior
\begin{equation}
  p(\theta_1 | y) \ = \ \int p(\theta_1 | \theta_2, y) \cdot p(\theta_2|y) \cdot d\theta_2
\end{equation}
I think this form makes it clearer what we're doing - we're taking the weighted sum of $\theta_1$'s posterior densities condtional on $\theta_2$, where the weights correspond to the posterior probabilities of the relevant value of $\theta_2$.
\par
Factoring the marginal posterior in this way also reveals a possible way to simulate it. We can first model the posterior distribution of $\theta_2$, draw a sample $\theta_2$, then use this to evaluate $p(\theta_1 | \theta_2, y)$ from which we can draw a value of $\theta_1$. If we repeat this process many times, the resulting distribution will be similar to $p(\theta_1 | y)$
\par
Now for an example. Say we're trying to model the mean $\mu$ of a Normally distributed random variable $Y$ for which we don't know the variance. $\sigma^2$. We can formulate the marginal posterior as
\begin{equation}
  p(\mu|y) = \int p(\mu | \sigma^2, y)\cdot p(\sigma^2 | y)\cdot d\sigma^2
\end{equation}
So if we're to follow the procedure outlined above in which we sample from $p(\sigma^2 |y)$, then from $p(\mu | \sigma^2, y)$, we'll need expressions for both of these terms. $p(\mu | \sigma^2, y)$ is quite straightforward:
\begin{align}
  p(\mu | \sigma^2, y) &\propto p(y | \mu, \sigma^2)\cdot p(\mu | \sigma^2) \\
  p(\mu | \sigma^2, y) &\propto \prod_{i = 1}^n \frac{1}{\sigma} \exp\Big({-\frac{(y^{(i)} - \mu)^2}{2\sigma^2}}\Big)
\end{align}
We assume that the location and scale are independent so that $p(\mu|\sigma^2) = p(\mu) = \text{Unif}(-\infty, \infty) = \texttt{constant}$. We also assume that the observations are independent.
\par
$p(\sigma^2 | y)$ is a little more tricky to derive. Again we use the fact that
\begin{equation}
  p(\sigma^2 | y) \propto p(y|\sigma^2) \cdot p(\sigma^2)
\end{equation}
For $p(y|\sigma^2)$ we can integrate $p(y|\sigma^2, \mu)$ over $\mu$
\begin{align}
  p(y|\sigma^2) &= \int \prod_{i = 1}^n \frac{1}{\sqrt{2\pi}\sigma}\exp{\Big( -\frac{(y^{(i)} - \mu)^2}{2\sigma^2} \Big)}d\mu \\
  &= \frac{1}{(2\pi\sigma^2)^{n/2}} \cdot \exp{\Big(-\frac{(n - 1)s^2}{2\sigma^2} \Big)} \cdot \int \exp{\Big(-\frac{n(\bar{y} - \mu)^2}{2\sigma^2}\Big)}d\mu  \\
    &= \frac{1}{(2\pi\sigma^2)^{n/2}} \cdot \exp{\Big(-\frac{(n - 1)s^2}{2\sigma^2} \Big)} \cdot \sqrt{2\pi}\frac{\sigma}{\sqrt{n}} \\
    &= \frac{1}{\sqrt{n}\cdot(2\pi\sigma^2)^{\frac{n-1}{2}}} \cdot \exp{\Big(-\frac{(n - 1)s^2}{2\sigma^2} \Big)}
\end{align}
To derive this we made use of the fact that the integral in (9) is Normal and that the sample variance of our observations is $s^2 = \frac{1}{n - 1}\sum_{i = 1}^n(y^{(i)} - \bar{y})$. We can take a noninformative prior on $\sigma^2$ so that 
\begin{equation}
  p(\sigma^2) \propto (\sigma^2)^{-1}
\end{equation} 
This prior is appropriate when the d.o.f. of the data dwarfs the d.o.f. of the usual conjugate prior. Using (12) and (11) we can now express (7):
\begin{equation}
 p(\sigma^2 | y) \propto (\sigma^2)^{-\frac{n+1}{2}} \cdot \exp{\Big(-\frac{(n - 1)s^2}{2\sigma^2} \Big)}
\end{equation}
This has the form of an inverse gamma function (i.e. $\frac{1}{\sigma^2} \sim \text{Gamma(a, b)}$). 
\par
Finally we're in a position where can do some modelling. To reiterate, the process we're going to follow is:

1. Sample $\sigma^2$ from $p(\sigma^2|y)$.

2. Use these values of $\sigma^2$ to draw samples of $\mu$ from $p(\mu | \sigma^2, y)$.

3. Chart the frequency of the $\mu$ values in a histogram. This distribution will be representative of the marginal posterior $p(\mu | y)$.

Before we do anything however, we'll need some data to work with. We'll use thirty data points drawn from a Normal distribution with a true mean of 50 and a true variance of 10.

```{r}
  set.seed(42)
  n_obs <- 10
  obs_y <- rnorm(n = n_obs, mean = 50, sd = sqrt(10))
  sample_var <- sum((obs_y - mean(obs_y))^2)/(n_obs-1)
```

Next we'll execute step (1) - drawing samples of $\sigma^2$ from $p(\sigma^2 | y)$.
```{r}
  sample_sigma_sq <- function(n_samples, n_obs, sample_var) {
                        sigma_sq <- seq(0.01, 100, 0.1)
                        pd <- (sigma_sq^(-(n_obs+1)/2))*exp(-sample_var*(n_obs - 1)/
                                                                        (2*sigma_sq))
                        samples <- sample(x = sigma_sq, size = n_samples,
                                          replace = TRUE, prob = pd)
                        return(samples)
  }

n_samples <- 5000
var_samples <- sample_sigma_sq(n_samples, n_obs, sample_var)
{hist(var_samples, breaks = 100, main = 'Marginal posterior of variance',
      xlab = expression(paste(sigma^2, ' value')), probability = TRUE, 
     ylab = 'Probability density', xlim = c(0, 40))
  
  # This is an (n - 1) d.o.f. inverse chi-square distributed r.v. scaled by (n-1)*sample_var
  sigma_sq <- seq(0.01, 100, 0.1)
  x <- (n_obs - 1)*sample_var/sigma_sq
  p_sigma <- dchisq(x = x, df = n_obs - 1)*x/sigma_sq
  lines(sigma_sq, p_sigma, xlim = c(0, 10), type = 'l')
}
```
We can plug these values of $\sigma^2$ into the expression for the conditional posterior distribution of $\mu$, then draw samples according to the resulting probability densities.
```{r}
  sampling_pd <- function(obs_y, var, mu) {
                        # Returns sampling probability density of observations under 
                        # possible mu values
                        # obs_y:  Vector of observations that are thought to be 
                        #         distributed Normally
                        # var:    Variance hyperparameter value
                        # mu:     Vector of mu values over which to compute p(mu|y, var)
                        sample_p <- sapply(obs_y, FUN = function(y) exp(-((y-mu)^2)/(2*var))/sqrt(var))
                        agg_sample_p <- apply(sample_p, MARGIN = 1,
                                               FUN = function (sample) prod(sample))
                        agg_sample_p <- agg_sample_p/sum(agg_sample_p*(max(mu) - min(mu))/length(mu))
                        return(agg_sample_p)
                        } 

sample_mu <- function(n_samples, obs_y, var_list) {
                        # Retrieves mu values from the conditional posterior of mu
                        # n_samples: Scalar number of samples to take from conditional
                        #            posterior
                        # obs_y:     Vector of observations that are thought to be
                        #            distributed Normally
                        # var_list:  Vector of variance samples from the marginal
                        #            posterior of sigma^2
                        mu <- seq(20, 80, 0.05)
                        sampled_mu <- matrix(0, nrow = length(var_list), ncol = 1)
                        i <- 1
                        for (var in var_list) {
                          # Compute p.d. of mu|y, sigma^2, then sample from it
                          cond_post_pd <- sampling_pd(obs_y, var, mu)
                          sampled_mu[i] <- sample(mu, size = n_samples, replace = TRUE,
                                                  prob = cond_post_pd)
                          i <- i + 1
                        }
                        return(sampled_mu)
}

# Check that the conditional posterior of mu is correct
# we'd expect it to be N(sample_mean, sigma_sq/n)
x <- seq(20, 80, 0.05)
approx_pd <- sampling_pd(obs_y, var = 20, mu = x)
true_pd <- dnorm(x, mean = mean(obs_y), sd = sqrt(20/n_obs))
{plot(x, approx_pd, main = 'Comparison of computed conditional posterior against analytical',
      xlim = c(40, 60))
lines(x, true_pd, col = 'blue')}
```
Having verified that the marginal distribution of the variance is scaled inverse Chi-Square and that the conditional posterior of the mean is Normal, we can now estimate the marginal posterior density of the mean. All being well, it should follow a t-distribution with $n-1$ degrees of freedom.
```{r}
mu_samples <- sample_mu(1, obs_y, var_samples)
h <- hist(mu_samples, breaks = seq(20, 80, 0.05), plot = FALSE)
{plot(h$breaks[-1], h$density,
     xlab = expression(paste(mu, ' value')),
     ylab = 'Probability density',
     main = 'Marginal Posterior Probability Density over Observation Mean',
     xlim = c(45, 55))
lines(seq(20, 80, 0.05), dt(x = (seq(20, 80, 0.05) - 
                                   mean(obs_y))/sqrt(sample_var/n_obs),
                            df = n_obs - 1), type = 'l')}
```
